% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Evaluation}\label{chapter:evaluation}
\section{Tasks to evaluate}

\subsection{Evaluation of the completeness and accuracy of the produced heuristics against a reference data set}
To evaluate the completeness and accuracy of the heuristics produced during the development of this thesis a reference data set has been used.\\
We implemented a procedure to check all files in the projects whether it contains a generator-pattern. These files are separately saved in a list and reviewed randomly for their amount of false positives. \\
Bernwieser~\cite{Bernwieser2014} provided a list of files that he detected as generated to us, to witch our results are compared against. \\
We do this to evaluate whether the approach given in this thesis is an improvement to the existing ones, especially the one given by~\cite{Bernwieser2014}. It resolves whether the generator-pattern repository can be used as a database in production while performing static code analysis.

\subsection{Automatic classification of source code in a huge collection of open source systems \& evaluation of the ratio of generated code}
To test the generator patterns stored in the repository a variety of open source projects have been acquired.\\
These projects have been filtered using the generator-pattern repository to detect the amount of generated code in a huge, randomly composed collection of open source systems.\\
We calculated the amount of generated code in these projects to evaluate the usage of code generators and their importance in programming. 

\section{Study Objects}
We used different study objects to test the approach used in this thesis and to evaluate the completeness and accuracy of the generator-pattern repository. \\
A reference data set is described in Section~\ref{section:qualitasCorpus} which includes good documented code, whereas Section~\ref{section:randomGitProjects} describes a randomly composed collection of open source projects that is used to test the generator-pattern repository in a not particularly curated environment.
 
\subsection{Qualitas Corpus}
\label{section:qualitasCorpus}
"The Qualitas Corpus is a curated collection of software systems intended to be used for empirical studies of code artefacts. The primary goal is to provide a resource that supports reproducible studies of software. The current release of the Corpus contains open-source Java software systems, often multiple versions." \cite{TemperoEwanandAnslowCraigandDietrichJensandHanTedandLiJingandLumpeMarkusandMeltonHaydenandNoble2010a}\\
Nonetheless the procedure is designed to be also applicable onto sets including other programming languages as well as sets of projects with mixed ones.\\
The current release is from the year 2013 and includes 112 different projects. To be easily comparable to the work in \cite{Bernwieser2014}, we also added the project Mahout~\cite{ApacheSoftwareFoundation}.\\
Additionally, we could perform the analysis on all projects so we didn't have to exclude \textit{eclipse\_SDK} nor \textit{netbeans}.\\
Table~\ref{table:qualitasCorpusAll} displays all projects that are included in the Qualitas Corpus together with their amount of lines of code. The size of the projects ranges from 6.991 \textit{(fitjava)} up to 7.142.778 \textit{(netbeans)}.
\input{tables/qualitasCorpusAll.tex}

\subsection{Random Git Projects}
\label{section:randomGitProjects}
To collect a huge, randomly composed collection of open source projects a git crawler has been implemented.
By passing a keyword to the crawler it queries \href{github.com}{GitHub} to return project descriptions of projects that contain the keyword in their names or descriptions. From the received project descriptions the links to clone the projects are extracted. The projects are cloned and saved sorted by their programming languages, whereas the crawler queries for projects written in all languages supported by Teamscale to provide the highest level of randomness and diversity for the projects.
\input{tables/gitProjects.tex}

\section{Study Design}
To evaluate the completeness and accuracy of the produced heuristics we used the Qualitas Corpus to calculate the amount of generated code. Using this information we compared our results to the ones provided in \cite{Bernwieser2014} to derive the improvement our approach makes in distinguishing generated from manually maintained code.\\
Based on the set of generated source code files we calculated different metrics:
\begin{itemize}
	\item Lines of code for generated and manually maintained source code.
	\item Number of generated and manually maintained source code files.
	\item Ratio of lines of code per source code files in the categories.
\end{itemize}

\section{Procedure}
This section justifies the decisions we made for the different thresholds as well as our decision to highly multithread the different steps used in the approach.

\subsection{Testing environment}
The benchmarks are performed on an \textit{Intel Core i7-6700HQ CPU} running with a frequency of \textit{2.60GHz} on \textit{four} physical cores with \textit{40GB RAM}. To run the Java Code we used the \textit{Eclipse IDE} \annotation{Cite} executed on \textit{Ubuntu 18.04.1 LTS (64-bit)} based on the \textit{4.15.0-33-generic} Kernel.\\
We compare the durations of the single steps performed during the suffix-tree clone-detection approach to find generator-patterns. We used the projects \textit{azureus, batik, checkstyle, cobertura, compiere, derby, drjava, exoportal, freecol, freecs, galleon, hsqldb, htmlunit, ireport, ivatagroupware, jFin\_DateMath, javacc, jedit, jgrapht, jhotdraw, jmoney, joggplayer, jparse, jspwiki, jstock, jung, maven, netbeans, openjms, oscache, quilt, sandmark, squirrel\_sql, tapestry, trove, weka, xerces} as a reference benchmark environment and ran the procedure several times to calculate a solid average value for the durations. \\
The decision for this environment is reasoned by its size of 37~projects including 60.235~source code files, whereas 3034~files are generated. It contains a total of 1.166.654~comments with a total of 23.683.887~valuable words. This pushes the machine the benchmark has been performed on to its maximum heap space it can provide to the Java Virtual Machine. The projects are chosen randomly to provide a meaningful mean of projects to preserve the generality of the Qualitas Corpus.

\subsection{Thresholds}
The threshold that has a direct impact on the number of found generator-patterns is the minimum clone length. It describes the minimal length a sequence of \code{CloneChunk}s has to provide to be considered by the Step~\ref{section:findClones}. If a sequence is shorter than the minimum clone length it will not be added to the list of clone classes independent of the size the clone class would have.\\
Finding the best minimum clone length has been done by performing tests with different clone lengths ranging from a length of 2\footnote{We didn't start at 1 because single words aren't meaningful when searching for generator-patterns. Especially words like \textit{the, a, by \dots} would have many occurrences that generate a huge amount of false positive clone classes.} up to a length of 25.\\
We finally decided on the minimum clone length of 5, where as the choice has been a mostly subjective one. Nonetheless did we consider the draw-off between the amount of results that are lost due to a to high minimum clone length and the amount of irrelevant data generated by a to low minimum clone length.\\
Figure~\ref{fig:thresholdsAll} shows all found \code{CloneResult}s associated to the minimum clone length used to find it. It can be seen that by increasing the minimum clone length the number of \code{CloneResult}s drops nearly exponentially. \\
After applying the processing steps described in Step~\ref{section:processCloneResults} to reduce the amount of data that is created, the remaining \code{CloneResult}s behave as shown in Figure~\ref{fig:thresholdsProcessed}. It shows that by filtering the amount of \code{CloneResult}s drops by around 99\%, dropping even further by accumulating and clustering.\\
We tried using the minimum clone length of 3 at first due to the peak in the remaining \code{CloneResult}s. Nonetheless did we decide to use 5 because the observation was that no generator-patterns were lost by using this minimum clone length, but the manual search for the patterns in the links was much easier because the sequences were more meaningful.
\input{figures/Thresholds/thresholdAll.tex}
\input{figures/Thresholds/thresholdProcessed.tex}

\subsection{Benchmarking}
\label{section:benchmark}
As displayed in Figure~\ref{fig:benchmark}, the time-intensive steps are multithreaded.\\
We performed the benchmark with a minimum clone length of 5.\\
This results in time savings ranging from 5.32\% \textit{(Get Uniform Paths)} up to 86.25\% \textit{(Create Links For: GENERATED)}. The overall time saving sums up to a total of 56.5\%, which reduces the average absolute amount of time of around 300 seconds down to 130 seconds. 
\input{figures/Benchmark/benchmark.tex}
\input{tables/tickLabels.tex}

\section{Results}
This section displays the results we were able to create by using the approach to find generator-patterns and by using the the resulting generator-pattern repository on the different data sets. 
\subsection{Qualitas Corpus}
At first we considered the Qualitas Corpus because it provided a good database and the results can be compared to the ones made in \cite{Bernwieser2014}.\\
We found a total of 41 from the 112 projects to contain generated code. 

\subsubsection{Number of generator-patterns found}
A total of 48 generator-patterns have been found. They are generated by 29 different generators. Additionally most of the patterns are regular expressions that accept up to 32 different patterns so that the final number of found generator patterns is much higher.

%\input{figures/QualitasCorpusComparison/comparison.tex}
\subsubsection{Lines of code for generated and manually maintained source code.}
\label{section:qualitasCorpusLOC}
The Table~\ref{table:locQualitasCorpus} shows how many \textit{lines of code} fall into the categories of generated and manually maintained code augmented with the ratios the categories cover. The ratio in lines of code for generated source code range from very low values \textit{(0,05\% - Hibernate)} up to more than three fourth \textit{(75,03\% - Cobertura)} of the overall lines of code.\\
Additionally, the Figure~\ref{fig:comparison} displays the accumulated ratios of generated and manually maintained lines of code.\\
The average proportion of generated lines of code is calculated using the equation
\begin{equation}
	\label{eq:locAvg}
	LOC_{avg} = \frac{\mathlarger{\sum^{projects} \frac{LOC_{generated}}{LOC}}}{|projects|} = 13.95\%
\end{equation}
The total proportion of generated lines of code is calculated via
\begin{equation}
	\label{eq:locTotal}
	LOC_{total} = \frac{\mathlarger{\sum^{projects} LOC_{generated}}}{\mathlarger{\sum^{projects} LOC}} = 7.96\%
\end{equation} 
The average and total proportions of manually maintained lines of code are described by the complementary probability.

\input{tables/locQualitasCorpus.tex}

\subsubsection{Number of generated and manually maintained source code files.}
Table~\ref{table:countQualitasCorpus} additionally displays the \textit{number of source code files} that are generated next to the ones that are manually maintained. The ratios the code categories cover are again associated to the absolute values, whereas they range from a very small amount of files \textit{(0,02\% - Hibernate)} up to around two third \textit{(68,18\% - SableCC)}.\\
Analog to the calculation of the total and the average ratios of generated to manually maintained lines of code as described in in Section~\ref{section:qualitasCorpusLOC} are the proportions of generated to manually maintained files calculated. The same equations~\ref{eq:locAvg}~and~\ref{eq:locTotal} are used, but LOC is replaced by the number of files. %Figure~\ref{fig:comparison} displays the ratios.\\
This resulted in the proportions of generated files of
\begin{equation}
	Files_{avg} = 8.07\%, \quad Files_{total} = 5.19\%
\end{equation} 
Again, the average and total proportions of manually maintained files are described by the complementary probability.

\input{tables/countQualitasCorpus.tex}

\subsubsection{Lines of code per source code file}
When looking at the Tables~\ref{table:countQualitasCorpus}~and~\ref{table:locQualitasCorpus} we saw that in many projects only a few generated files are responsible for a high number in lines of code.\\
We decided to calculate the proportion of lines of code per file for the generated and the manually maintained files and could confirm that there exists indeed a correlation between the file size and the possible generation as stated in~\cite{Bernwieser2014}.\\
We calculated a \textbf{total} ratio of generated lines of code to generated files of 
\begin{equation}
	\frac{LOC_{generated}}{Files_{generated}} = 321 , \quad \frac{LOC_{manually}}{Files_{manually}} = 204
\end{equation}
and an \textbf{average} ratio of
\begin{equation}
	\frac{LOC_{generated}}{Files_{generated}} = 694 , \quad \frac{LOC_{manually}}{Files_{manually}} = 214
\end{equation}

\subsection{Random Git Projects}
We started the search for generated code in the huge, randomly composed collection of open source projects by again applying the algorithm created for this thesis. In the first iteration we filtered the source code files using the generator-patterns found in the Qualitas Corpus. In the second iteration we used all generator-patterns found in the collection. \\
The Tables~\ref{table:locGit} and~\ref{table:countGit} show the results of the first and the second iteration. The column \textit{Only Qualitas Corpus} refers to the first iteration using only the generator-patterns found in the Qualitas Corpus to filter the files. The column \textit{Git Patterns added} describes the second iteration using all patterns found in the open source projects.

\subsubsection{Number of generator-patterns found}
We found a total of 34 new generator patterns that are widely distributed over the different programming languages. These are again regular expressions that accept a multitude of different patterns so that the final number of found generator patterns is much higher.

\subsubsection{Lines of code for generated and manually maintained source code.}
Table~\ref{table:locGit} shows the results for the generated lines of code gathered during the two iteration steps.\\
In the first iteration the proportion of generated lines of code range from 0.01\% for the Swift projects up to 8.88\% for the Java projects. This result can be explained as the Qualitas Corpus contains only projects written in Java, so the previously found generator-patterns do apply best for the Java projects. Nonetheless some of the patterns are universal and are created by code generators that are able to generate code for different programming languages.\\
The total and average proportions for the lines of code are calculated analog to the procedure for the Qualitas Corpus and result in
\footnote{
	\label{note:onlyGenerated}
	We include only the projects that do contain generated code so the results are better comparable to the ones gathered for the Qualitas Corpus.
}:
\begin{equation}
	\label{eq:locGit_1}
	LOC_{avg} = 2.60\%, \quad LOC_{total} = 3.23\%
\end{equation}
Again, the average and total proportions of manually maintained files are described by the complementary probability.
In the second iteration the proportion of generated code found by using the patterns included in the collection raises enormously. Especially in the projects not written in Java the amount of detected generated code is utterly enlarged.\\
Again we calculated the total and average proportions and found the following\footref{note:onlyGenerated}:
\begin{equation}
\label{eq:locGit_2}
LOC_{avg} = 17.66\%, \quad LOC_{total} = 12.88\%
\end{equation}
Again, the average and total proportions of manually maintained files are described by the complementary probability.
\input{tables/locGit.tex}

\subsubsection{Number of generated and manually maintained source code files.}
Table~\ref{table:countGit} shows the results for the generated lines of code gathered during the two iteration steps.\\
The total and average proportions for the amount of generated source code files are again calculated using the equations used for the Qualitas Corpus.
For the first iteration this results in the proportions of generated files of
\begin{equation}
\label{eq:countGit_1}
	Files_{avg} = 1.32\%, \quad Files_{total} = 2.15\%
\end{equation} 
For the second iterations the results are 
\begin{equation}
\label{eq:countGit_2}
Files_{avg} = 8.97\%, \quad Files_{total} = 8.68\%
\end{equation} 
\input{tables/countGit.tex}

\subsubsection{Lines of code per source code file}
As a last point concerning the open source projects we calculated again the ratio of lines of code per source code file for the two iterations.\\
In the first iteration we found the total ratios of of:
\begin{equation}
	\frac{LOC_{generated}}{Files_{generated}} = 329 , \quad \frac{LOC_{manually}}{Files_{manually}} = 217
\end{equation}
and an \textbf{average} ratio of
\begin{equation}
	\frac{LOC_{generated}}{Files_{generated}} = 1948 , \quad \frac{LOC_{manually}}{Files_{manually}} = 228
\end{equation}
In the second iteration the ratios changed to:
\begin{equation}
\frac{LOC_{generated}}{Files_{generated}} = 293 , \quad \frac{LOC_{manually}}{Files_{manually}} = 215
\end{equation}
and an \textbf{average} ratio of
\begin{equation}
\frac{LOC_{generated}}{Files_{generated}} = 522 , \quad \frac{LOC_{manually}}{Files_{manually}} = 198
\end{equation}

\section{Discussion}
By analyzing the projects included in the Qualitas Corpus we found a wide variety of generator-patterns and were able to calculate the average and total proportions of generated to manually maintained lines of code and files. We saw that generated code makes only a small portion of the all lines of code, whereas in average a project contains 13.95\% generated lines of code and all projects contained 7.96\% generated lines of code.\\
Additionally we saw that in average 8.07\% of files are generated whereas in total only 5.19\% are generated.\\
We were able to find even more generator-patterns and resulting a higher number of generated source code after applying the algorithm on the huge set of open source projects. Hereby we found that the projects do contain even more generated code with a total of 12.88\% and an average of 17.66\% of the lines of code being generated. Also the total amount of files has risen to 8.97\% and the total amount to 8.68\%.\\
Resulting from that we showed that in average the generated files always contain more lines of code than the manually maintained ones, whereas there exists no direct correlation of the actual lines of code ratios between generated and manually maintained source code files.

\subsection{Completeness and accuracy}
We found the generator-patterns by iterating over the Qualitas Corpus and applying the algorithm described in Chapter~\ref{chapter:approach}. The found patterns had been added to the generator-pattern repository and used to filter all files that we were able to classify as generated in the next iteration.\\
In every iteration the amount of possible generator-patterns could be lowered by using this technique very fast until all generator-patterns were reviewed by hand and so all patterns had been found in the Qualitas Corpus.\\
We compared our results to these described in \cite{Bernwieser2014} and could confirm that our approach found all files and even. Resulting from this we concluded that our approach is as complete as it can be using the technique of suffix-tree clone-detection on the comments.\\
By using the approach on the huge random collection of open source projects we detected some more generator-patterns that were not included in the Qualitas Corpus. Nonetheless we were able to filter many generated files in the first iteration by using the generator-pattern repository what supports our assertion that the algorithm found all generator patterns that were included in the Qualitas Corpus.\\
Additionally, the generator-pattern repository is easily extendable with new patterns at any time what was shown by applying the algorithm on the collection of open source projects. So by gradually extending the repository by applying the suffix-tree clone-detection approach on the comments, the repository will saturate with patterns and will over time become complete and highly accurate in terms of generator-patterns.

\subsection{Relevance of results}

\annotation{Not sure what to write in this section.\\ Maybe how the generator-pattern repository can be used by Teamscale to lower amount of work done in analysis by excluding the generated files?}

\section{Threats to validity}
This section describes the threads that may have an impact on the validity of the results generated by our approach and justifies how .
\subsection{Wrong filtering}
In Step~\ref{section:filterCloneResults} we used the regular expression shown in Listing~\ref{lst:generatorPattern}. This reduces the amount of resulting \code{CloneResult}s to a level that can be processed by a human. Our observations showed that indeed most of the generator-patterns contained the regex so that it can be considered as only a small thread.\\
Nonetheless did we found generator-patterns that didn't contain the regex. These patterns make only a very small portion of all generator-patterns so we didn't see a value in refining it. Furthermore does step~\ref{section:generateLinks} also save files and links for the not generated found \code{CloneClass}es that can be reviewed in addition to the generated ones. The problem arising is that the amount of data to review is way to high as i could be process by hand, but the ones that had the highest number of occurrences were also searched by us what revealed the patterns that didn't include the regular expression.\\
In conclusion we can say that the regular expression is a very good filter to find the generator-patterns which can be easily extended to also review the patterns that were filtered out to find all generator-patterns.

\subsection{Minimum clone length vs. irrelevant data}
We faced the decision on a most optimal minimum clone length while applying the suffix-tree clone-detection algorithm made by Esko Ukkonen~\cite{Ukkonen1995} and the search for clones on the resulting tree structure. We finally decided on a length of 5. \\
The draw-off we faced was the one that a high minimum clone length resulted in a low number of \code{CloneResult}s. Hereby were all short generator-patterns sorted out before we were even aware that they existed in the code base.\\
On the other hand was a low minimum clone length not really much applicable due to the fact that it generated really much false positives like only the words \textit{generated, modify, edit}, whereas they do not hold any value without the context they were used in. Especially the word generated is often used in the context that the following method generates some sort of date, not that the method itself is generated.\\
Therefore we did a benchmark iterating over the minimum clone length and comparing the amount of data the different lengths generated and the subjective observation how useful the data was. In Figures~\ref{fig:thresholdsAll} we saw that the unfiltered \code{CloneResult}s do follow a nearly exponential descend, whereas a the minimum clone length of 5 creates only around 15\% compared to a length of 2. This arises the question whether a minimum clone length of 5 erases to much valuable data.\\
When you look at Figure~\ref{fig:thresholdsProcessed} we saw that by applying the three processing steps of filtering by the regular expression described in Step~\ref{section:filterCloneResults}, then accumulating the \code{CloneResult}s as stated in Step~\ref{section:accumulation} and finally clustering the results explained in Step~\ref{section:clustering} resulted in an amount of found \code{CloneResult}s having it's peak at the minimum clone length of 3. Nonetheless we decided on a length of 5 due to the fact that it erases only around 20\% in the final presumable generator-patterns generated, but it generates patterns that are much more speaking for themselves due to the extended length.\\
Additionally, we saw that the patterns which are lost by increasing the minimum clone length up to the length of 5 are all included in the patterns we found also by using the higher length. This is due to the fact that the shorter presumable patterns do overlap with the patterns found with the length of 5, but are no direct substrings of the longer generator-patterns. Nonetheless they do point to the same classes and in fact to the same patterns.\\
Following these thoughts we found the decided on a minimum clone length of 5.

\subsection{Representativeness of data sets}
We used the Qualitas Corpus as a reference set because it is declared as "intended to be used for empirical studies of code artefacts. The primary goal is to provide a resource that supports reproducible studies of software" \cite{TemperoEwanandAnslowCraigandDietrichJensandHanTedandLiJingandLumpeMarkusandMeltonHaydenandNoble2010a}. It is used in a variety of other studies and the results can be easily compared.\\
As we showed it includes a high number of generated code and also many generator-patterns, what makes it a good starting point to begin working with the created algorithm. Furthermore it consists of a high number of different projects (112) that are all maintained by big companies like \textit{Apache Software Foundation, Eclipse} or \textit{NetBeans} \annotation{Cite!!} and used in production. Due to the fact that \textit{CQSE GmbH} and their product \textit{Teamscale} is mostly used by big international companies to monitor their code quality, the parallels found between the projects led us to the conclusion that the Qualitas Corpus is a good starting point for the use in \textit{Teamscale}.\\
Nonetheless the problem we see in using the Qualitas Corpus is that it consists only of projects written in Java, which makes it only representative for this particular programming language. To avoid this problem we additionally used the algorithm on a huge randomly composed collection of open source projects we gathered from \href{github.com}{GitHub} we made a second, wide spread reference dataset developed by a huge number of independent programmers. This dataset is composed of very small up to really big projects that are written in a multitude of programming languages and developed all around the globe.\\
This led us to the conclusion that in conjunction the two reference datasets give a representative profile of very much, very different projects that are all developed under very unique circumstances.

\subsection{Generators without pattern}
A last thread we are facing is that there exists also a multitude of code generators that do not mark their artifacts with a characteristic pattern in the comments. Especially custom written generators often don't add a generator pattern.\\
This problem can't be faced with the current approach of suffix-tree clone-detection on the comments. To find these generated artifacts one has to combine our approach with e.g. the one described in \cite{Bernwieser2014}.