% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Evaluation}\label{chapter:evaluation}
\section{Tasks to evaluate}

\subsection{Evaluation of the completeness and accuracy of the produced heuristics against a reference data set}
To evaluate the completeness and accuracy of the heuristics produced during the development of this thesis a reference data set has been used.\\
We implemented a procedure to check all files in the projects whether it contains a generator-pattern. These files are separately saved in a list and reviewed randomly for their amount of false positives. \\
Bernwieser~\cite{Bernwieser2014} provided a list of files that he detected as generated to us, to witch our results are compared against. \\
We do this to evaluate whether the approach given in this thesis is an improvement to the existing ones, especially the one given by~\cite{Bernwieser2014}. It resolves whether the generator-pattern repository can be used as a database in production while performing static code analysis.

\subsection{Automatic classification of source code in a huge collection of open source systems \& evaluation of the ratio of generated code}
To test the generator patterns stored in the repository a variety of open source projects have been acquired.\\
These projects have been filtered using the generator-pattern repository to detect the amount of generated code in a huge, randomly composed collection of open source systems.\\
We calculated the amount of generated code in these projects to evaluate the usage of code generators and their importance in programming. 

\section{Study Objects}
We used different study objects to test the approach used in this thesis and to evaluate the completeness and accuracy of the generator-pattern repository. \\
A reference data set is described in Section~\ref{section:qualitasCorpus} which includes good documented code, whereas Section~\ref{section:randomGitProjects} describes a randomly composed collection of open source projects that is used to test the generator-pattern repository in a not particularly curated environment.
 
\subsection{Qualitas Corpus}
\label{section:qualitasCorpus}
"The Qualitas Corpus is a curated collection of software systems intended to be used for empirical studies of code artefacts. The primary goal is to provide a resource that supports reproducible studies of software. The current release of the Corpus contains open-source Java software systems, often multiple versions." \cite{TemperoEwanandAnslowCraigandDietrichJensandHanTedandLiJingandLumpeMarkusandMeltonHaydenandNoble2010a}\\
Nonetheless the procedure is designed to be also applicable onto sets including other programming languages as well as sets of projects with mixed ones.\\
The current release is from the year 2013 and includes 112 different projects. To be easily comparable to the work in \cite{Bernwieser2014}, we also added the project Mahout~\cite{ApacheSoftwareFoundation}.\\
Additionally, we could perform the analysis on all projects so we didn't have to exclude \textit{eclipse\_SDK} nor \textit{netbeans}.\\
Table~\ref{table:qualitasCorpusAll} displays all projects that are included in the Qualitas Corpus together with their amount of lines of code. The size of the projects ranges from 6.991 \textit{(fitjava)} up to 7.142.778 \textit{(netbeans)}.
\input{tables/qualitasCorpusAll.tex}
%the same two subsets of projects have been reused. The classification of the projects included in the two sets has been made in \cite{Bernwieser2014}, whereas one consists of projects including generated code and the other fully excluding this type of source code.\\
%Both collections show a very wide variation range regarding their project sizes; projects with generated code range from 35.388 \textit{(SableCC)} to 1.540.009 \textit{(GT2)}, projects without generated code from 29.587 \textit{(Informa)} to 645.715 \textit{(Jtopen)} \cite{Bernwieser2014}. 
%The project \textit{Mahout} has been replaced by \textit{Xalan} because it isn't included in the Qualitas Corpus anymore.\\ 
%The distribution of the projects over the environments is shown in Table~\ref{table:qualitasCorpusOverview}.
%\input{tables/qualitasCorpusOverview.tex}

\subsection{Random Git Projects}
\label{section:randomGitProjects}
To collect a huge, randomly composed collection of open source projects a git crawler has been implemented.
By passing a keyword to the crawler it queries \href{github.com}{GitHub} to return project descriptions of projects that contain the keyword in their names or descriptions. From the received project descriptions the links to clone the projects are extracted. The projects are cloned and saved sorted by their programming languages, whereas the crawler queries for projects written in all languages supported by Teamscale to provide the highest level of randomness and diversity for the projects.
\annotation{Describe more}

\section{Study Design}
\annotation{This section describes how the study, using the information from the study objects, attempts to answer the research questions.}

To evaluate the completeness and accuracy of the produced heuristics we used the Qualitas Corpus to calculate the amount of generated code. Using this information we compared our results to the ones provided in \cite{Bernwieser2014} to derive the improvement our approach makes in distinguishing generated from manually maintained code.\\
Based on the set of generated source code files we calculated different metrics:
\begin{itemize}
	\item Lines of code for generated and manually maintained source code.
	\item Number of generated and manually maintained source code files.
	\item Ratio of generated to manually maintained source code and files.
	\item Amount of generated code detected by our approach compared to the one used in \cite{Bernwieser2014}.
\end{itemize}

\section{Procedure}
This section justifies the decisions we made for the different thresholds as well as our decision to highly multithread the different steps used in the approach.

\subsection{Testing environment}
The benchmarks are performed on an \textit{Intel Core i7-6700HQ CPU} running with a frequency of \textit{2.60GHz} on \textit{four} physical cores with \textit{40GB RAM}. To run the Java Code we used the \textit{Eclipse IDE} \annotation{Cite} executed on \textit{Ubuntu 18.04.1 LTS (64-bit)} based on the \textit{4.15.0-33-generic} Kernel.\\
We compare the durations of the single steps performed during the suffix-tree clone-detection approach to find generator-patterns. We used the projects \textit{azureus, batik, checkstyle, cobertura, compiere, derby, drjava, exoportal, freecol, freecs, galleon, hsqldb, htmlunit, ireport, ivatagroupware, jFin\_DateMath, javacc, jedit, jgrapht, jhotdraw, jmoney, joggplayer, jparse, jspwiki, jstock, jung, maven, netbeans, openjms, oscache, quilt, sandmark, squirrel\_sql, tapestry, trove, weka, xerces} as a reference benchmark environment and ran the procedure several times to calculate a solid average value for the durations. \\
The decision for this environment is reasoned by its size of 37~projects including 60.235~source code files, whereas 3034~files are generated. It contains a total of 1.166.654~comments with a total of 23.683.887~valuable words. This pushes the machine the benchmark has been performed on to its maximum heap space it can provide to the Java Virtual Machine. The projects are chosen randomly to provide a meaningful mean of projects to preserve the generality of the Qualitas Corpus.

\subsection{Thresholds}
The threshold that has a direct impact on the number of found generator-patterns is the minimum clone length. It describes the minimal length a sequence of \code{CloneChunk}s has to provide to be considered by the Step~\ref{section:findClones}. If a sequence is shorter than the minimum clone length it will not be added to the list of clone classes independent of the size the clone class would have.\\
Finding the best minimum clone length has been done by performing tests with different clone lengths ranging from a length of 2\footnote{We didn't start at 1 because single words aren't meaningful when searching for generator-patterns. Especially words like \textit{the, a, by \dots} would have many occurrences that generate a huge amount of false positive clone classes.} up to a length of 25.\\
We finally decided on the minimum clone length of 5, where as the choice has been a mostly subjective one. Nonetheless did we consider the draw-off between the amount of results that are lost due to a to high minimum clone length and the amount of irrelevant data generated by a to low minimum clone length.\\
Figure~\ref{fig:thresholdsAll} shows all found \code{CloneResult}s associated to the minimum clone length used to find it. It can be seen that by increasing the minimum clone length the number of \code{CloneResult}s drops nearly exponentially. \\
After applying the processing steps described in Step~\ref{section:processCloneResults} to reduce the amount of data that is created, the remaining \code{CloneResult}s behave as shown in Figure~\ref{fig:thresholdsProcessed}. It shows that by filtering the amount of \code{CloneResult}s drops by around 99\%, dropping even further by accumulating and clustering.\\
We tried using the minimum clone length of 3 at first due to the peak in the remaining \code{CloneResult}s. Nonetheless did we decide to use 5 because the observation was that no generator-patterns were lost by using this minimum clone length, but the manual search for the patterns in the links was much easier because the sequences were more meaningful.
\input{figures/Thresholds/thresholdAll.tex}
\input{figures/Thresholds/thresholdProcessed.tex}

\subsection{Benchmarking}
\label{section:benchmark}
As displayed in Figure~\ref{fig:benchmark}, the time-intensive steps are multithreaded.\\
We performed the benchmark with a minimum clone length of 5.\\
This results in time savings ranging from 5.32\% \textit{(Get Uniform Paths)} up to 86.25\% \textit{(Create Links For: GENERATED)}. The overall time saving sums up to a total of 56.5\%, which reduces the average absolute amount of time of around 300 seconds down to 130 seconds. 
\input{figures/Benchmark/benchmark.tex}
\input{tables/tickLabels.tex}

\section{Results}
This section displays the results we were able to create by using the approach to find generator-patterns and by using the the resulting generator-pattern repository on the different data sets. 
\subsection{Qualitas Corpus}
At first we considered the Qualitas Corpus because it provided a good database and the results can be compared to the ones made in \cite{Bernwieser2014}.

\input{tables/locQualitasCorpus.tex}
\input{tables/countQualitasCorpus.tex}

\subsubsection{Lines of code for generated and manually maintained source code.}
- 
\subsubsection{Number of generated and manually maintained source code files.}
- 
\subsubsection{Ratio of generated to manually maintained source code and files.}
- 
\subsubsection{Amount of generated code detected by our approach compared to the one used in \cite{Bernwieser2014}.}
- 
\subsection{Random Git Projects}
a
\section{Discussion}
a
\subsection{Completeness and accuracy}
a
\subsection{Relevance of results}
a
\section{Threats to validity}
a
\subsection{Wrong filtering}
a
\subsection{Minimum clone length vs. irrelevant data}
a
\subsection{Representativeness of data sets}
a
\subsection{Generators without pattern}
a